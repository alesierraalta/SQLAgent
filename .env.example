# Database Configuration
DATABASE_URL=postgresql://user:password@localhost:5432/dbname

# OpenAI Configuration
OPENAI_API_KEY=your_openai_api_key_here
OPENAI_MODEL=gpt-4o

# Schema Configuration
SCHEMA_DISCOVERY=true
SCHEMA_TTL_SECONDS=300  # Schema cache TTL in seconds (default: 300 = 5 minutes)

# Cache Configuration
CACHE_TTL_SECONDS=3600  # Query result cache TTL in seconds (default: 3600 = 1 hour)
USE_REDIS_CACHE=false   # Cambia a true si tienes Redis disponible

# Persistent Cache Configuration (Fase C)
CACHE_BACKEND=memory    # Cache backend: memory, file, or redis (default: memory; set to redis for Docker)
CACHE_DIR=.data/cache   # Directory for file cache (only used if CACHE_BACKEND=file)
REDIS_URL=redis://localhost:6379/0  # Redis URL (used when CACHE_BACKEND=redis or USE_REDIS_CACHE=true)
REDIS_HOST=localhost
REDIS_PORT=6379
REDIS_DB=0
REDIS_PASSWORD=

# ML Classification Configuration (Fase E)
USE_ML_CLASSIFICATION=true  # Use ML-based query classification (default: true)
EMBEDDING_MODEL=all-MiniLM-L6-v2  # Sentence transformer model (default: all-MiniLM-L6-v2)

# OpenTelemetry Configuration (Fase F)
ENABLE_TELEMETRY=false  # Enable OpenTelemetry metrics and tracing (default: false)
SERVICE_NAME=llm-data-warehouse  # Service name for telemetry
SERVICE_VERSION=1.0.0  # Service version
